{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8f15b0f9-3b7b-42cb-81fe-4ada30017540",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0 \n",
    "Copyright (c) 2024, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>\n",
    "Ray Bernard <ray.bernard@outlooi.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d52c0da-f19e-4655-8164-33268ad1ab76",
   "metadata": {},
   "source": [
    "## Finetuning LLaMa3 8B Instruct on Intel Max Series GPUs 🚀\n",
    "Intel® Data Center Max 1100 GPU: A High-Performance 300-Watt Double-Wide AIC Card\n",
    "Featuring 56 Xe cores and 48 GB of HBM2E memory, the Intel® Data Center Max 1100 GPU delivers powerful performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c507aa8c-33fc-4832-971d-f7c08e33b62e",
   "metadata": {},
   "source": [
    "### Step 1: Initial Setup\n",
    "\n",
    "Run this step only once to ensure you have the proper libraries installed. Additionally, make sure to use the Modin kernel!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acfcc83-5e4a-41e8-9fd0-20a0a61e2c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "import subprocess\n",
    "import shutil\n",
    "\n",
    "# Uninstall the invalid distributions if they are partially installed\n",
    "subprocess.run([\"pip\", \"uninstall\", \"-y\", \"torch\", \"transformers\"])\n",
    "\n",
    "# Clean up the site-packages directory\n",
    "site_packages_dir = os.path.expanduser(\"~/.local/lib/python3.9/site-packages\")\n",
    "\n",
    "def remove_directory(dir_path):\n",
    "    if os.path.exists(dir_path):\n",
    "        shutil.rmtree(dir_path, ignore_errors=True)\n",
    "\n",
    "torch_dirs = [os.path.join(site_packages_dir, d) for d in os.listdir(site_packages_dir) if d.startswith(\"torch\")]\n",
    "transformers_dirs = [os.path.join(site_packages_dir, d) for d in os.listdir(site_packages_dir) if d.startswith(\"transformers\")]\n",
    "\n",
    "for dir_path in torch_dirs + transformers_dirs:\n",
    "    remove_directory(dir_path)\n",
    "\n",
    "print(\"Cleaned up invalid directories.\")\n",
    "# Clear the pip cache\n",
    "print(\"Clearing the pip cache...\")\n",
    "subprocess.run([\"pip\", \"cache\", \"purge\"])\n",
    "\n",
    "cache_dir = result.stdout.strip()\n",
    "\n",
    "print(f\"Cache directory: {cache_dir}\")\n",
    "\n",
    "# Check the size of the cache directory\n",
    "result = subprocess.run([\"du\", \"-sh\", cache_dir], capture_output=True, text=True)\n",
    "print(f\"Cache size: {result.stdout}\")\n",
    "\n",
    "# Install the required packages\n",
    "!{sys.executable} -m pip install torch==2.1.0.post2 torchvision==0.16.0.post2 torchaudio==2.1.0.post2 intel-extension-for-pytorch==2.1.30+xpu oneccl_bind_pt==2.1.300+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n",
    "!{sys.executable} -m pip install --upgrade  \"transformers>=4.38.*\"\n",
    "!{sys.executable} -m pip install --upgrade  \"datasets>=2.18.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"wandb>=0.16.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"trl>=0.7.11\"\n",
    "!{sys.executable} -m pip install --upgrade \"peft>=0.9.0\"\n",
    "!{sys.executable} -m pip install --upgrade \"accelerate>=0.28.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"joblib\"\n",
    "!{sys.executable} -m pip install --upgrade \"threadpoolctl\"\n",
    "\n",
    "\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4008feb3-d77e-4161-88b0-965254ce4e49",
   "metadata": {},
   "source": [
    "### Step 2: Check Intel XPU Availability and Retrieve Device Capabilities\n",
    "\n",
    "In this step, we will import necessary libraries, check the availability of Intel XPU (eXtreme Performance Unit), and retrieve detailed device capabilities. This ensures that our environment is correctly configured to leverage the Intel XPU for optimal performancnt available\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd3cb4e-d4a2-44ab-8c25-8cd1c1db73ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import json \n",
    "\n",
    "# Check if Intel XPU is available\n",
    "if torch.xpu.is_available():\n",
    "    print(\"Intel XPU is available\")\n",
    "    for i in range(torch.xpu.device_count()):\n",
    "        print(f\"XPU Device {i}: {torch.xpu.get_device_name(i)}\")\n",
    "    \n",
    "    # Get the device capability details\n",
    "    device_capability = torch.xpu.get_device_capability()\n",
    "    \n",
    "    # Convert the device capability details to a JSON string with indentation for readability\n",
    "    readable_device_capability = json.dumps(device_capability, indent=4)\n",
    "    \n",
    "    # Print the readable JSON\n",
    "    print(\"Detail of GPU capability =\\n\", readable_device_capability)\n",
    "else:\n",
    "    print(\"Intel XPU is not available\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3367f8cc-9320-4588-a2ee-662378fed998",
   "metadata": {},
   "source": [
    "### Step 3: Optimize Environment for Intel Max Series GPUs\n",
    "\n",
    "To optimize performance when using Intel Max Series GPUs:\n",
    "\n",
    "1. **Suppress Warnings**: Import the `warnings` module and configure it to ignore unnecessary warnings.\n",
    "2. **Import Required Modules**: Use the `os` and `psutil` modules for setting environment variables and retrieving CPU information.\n",
    "3. **Retrieve CPU Information**: Determine the number of physical CPU cores and calculate cores per socket using `psutil`.\n",
    "4. **Set Environment Variables**:\n",
    "   - Disable tokenizers parallelism.\n",
    "   - Improve memory allocation with `LD_PRELOAD` (optional).\n",
    "   - Reduce GPU command submission overhead.\n",
    "   - Enable SDP fusion for efficient memory usage.\n",
    "   - Configure OpenMP to use physical cores, bind threads, and set thread pinning.\n",
    "5. **Print Configuration**: Display the number of physical cores, cores per socket, and OpenMP environment variables to verify the settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff51abb-9ca0-4243-b426-6e6dc407e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Set the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not available, please comment this line out\n",
    "os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0759a6-9014-4615-bb7b-017dc51b1a08",
   "metadata": {},
   "source": [
    "### Step 4: Monitor XPU Memory Usage in Real-Time\n",
    "\n",
    "The following script sets up a real-time monitoring system that continuously displays the XPU memory usage in a Jupyter notebook, helping you keep track of resource utilization during model training and inference. This setup helps in maintaining optimal performance and preventing resource-related issues during your deep learning tasks.  By keeping track of memory usage, you can prevent out-of-memory errors, optimize resource allocation, and ensure smooth training and inference processes. By monitoring these metrics, you can predict out-of-memory issues. If memory usage approaches the hardware limits, it’s an indication that the model or batch size might need adjusted etc.\n",
    "   - **Memory Reserved**: Indicates the total memory reserved by the XPU. Helps in understanding the memory footprint of the running processes.\n",
    "   - **Memory Allocated**: Shows the actual memory usage by tensors, crucial for identifying memory leaks or excessive usage.\n",
    "   - **Max Memory Reserved/Allocated**: These metrics help in identifying peak memory usage, which is essential for planning and scaling your models.\n",
    "   - performance and preventing resource-related issues during your deep learning tasks.eemory_monitor(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "577cc356-6cec-411e-b258-22d0e7eb9202",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>XPU (Intel(R) Data Center GPU Max 1100) :: Memory: Reserved=0.0 GB, Allocated=0.0 GB, Max Reserved=0.0 GB, Max Allocated=0.0 GB</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import asyncio\n",
    "import threading\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import torch\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\"XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "    \n",
    "    async def display_memory_usage(output):\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        output.update(HTML(f\"<p>XPU Name: {device_name}</p>\"))\n",
    "        while True:\n",
    "            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "            memory_usage_text = f\"XPU ({device_name}) :: Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "            output.update(HTML(f\"<p>{memory_usage_text}</p>\"))\n",
    "            await asyncio.sleep(5)\n",
    "    \n",
    "    def start_memory_monitor(output):\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.create_task(display_memory_usage(output))\n",
    "        thread = threading.Thread(target=loop.run_forever)\n",
    "        thread.start()    \n",
    "    output = display(display_id=True)\n",
    "    start_memory_monitor(output)\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a018a0-f1a1-42c0-8041-f0d9d9ed6669",
   "metadata": {},
   "source": [
    "## Step 5 Log into your hugging face account and enter your access token.  \n",
    "Uncheck the Add token as git credential! 🎛️\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "19af72a5-9b23-4fe5-8ca8-bf45c7bdf7dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "386f3b88cdc24888ae46a0c2e2e4794f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#loggin to huggnigface\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1be4c1c-3527-4f20-a7ce-4605831fd6ae",
   "metadata": {},
   "source": [
    "## Step 6 Configure LoRA for Efficient Training 🎛️\n",
    "\n",
    "In this step, we configure the LoRA (Low-Rank Adaptation) settings for efficient training of our model. LoRA is a technique that improves the efficiency of training by reducing the number of parameters through low-rank decomposition. Here, we instantiate a LoraConfig object with specific parameters tailored to our training needs.\n",
    "\n",
    "Instantiate LoRA Configuration:\n",
    "- r: Set to 32, this parameter controls the dimension of the low-rank decomposition, balancing model capacity and efficiency.\n",
    "- lora_alpha: Set to 16, this scaling factor adjusts the output of the low-rank decomposition, influencing the strength of the adaptation.\n",
    "- lora_dropout: Set to 0.5, this dropout rate applies regularization to the LoRA layers to prevent overfitting. A higher value increases regularization.\n",
    "- bias: Set to \"none\", indicating no bias is added to the LoRA layers.\n",
    "- target_modules: Specifies the layers where the low-rank adaptation will be applied. Here, it includes \"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\".\n",
    "- task_type: Set to \"CAUSAL_LM\", indicating that this configuration is for a causal language modeling task.\n",
    "- This configuration optimizes the model's training efficiency and performance by carefully adjusting the parameters and specifying the target modules for low-rank adaptation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0876949a-2abc-482e-beed-01ec20eace82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "# Instantiate a LoraConfig object with specific parameters\n",
    "lora_config = LoraConfig(\n",
    "    r=32,  # The dimension of the low-rank decomposition. This parameter controls the trade-off between model capacity and efficiency.\n",
    "    lora_alpha=16,  # The scaling factor for the LoRA module. It is used to adjust the output of the low-rank decomposition.\n",
    "    lora_dropout=0.5,  # The dropout rate applied to the LoRA layers to prevent overfitting. A higher value means more regularization.\n",
    "    bias=\"none\",  # Specifies how to handle biases in the LoRA layers. \"none\" means no bias is added.\n",
    "    \n",
    "    # The target modules for the LoRA transformation. These are the specific layers in the model where the low-rank adaptation will be applied.\n",
    "    # You could use 'q_proj', 'v_proj', and '0_proj' as well and comment out the rest if needed.\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"output_proj\"],  \n",
    "    \n",
    "    task_type=\"CAUSAL_LM\"  # Specifies the task type for which this configuration is being used. \"CAUSAL_LM\" stands for causal language modeling.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7f5084-3af2-43d4-8372-96e7be52ee35",
   "metadata": {},
   "source": [
    "### Step 7: Load and Prepare the Model\n",
    "\n",
    "In this step, we ensure the model is loaded and prepared for use on the appropriate device, either an Intel XPU or CPU, and configure it for efficient fine-tuningThis ensures the model and tokenizer are properly set up and optimized for use on the selected device, ready for efficient fine-tuning.\n",
    "This step ensures that the model and tokenizer are correctly set up and configured for use on the appropriate device, preparing them for the fine-tuning process.\n",
    ".\n",
    "\n",
    "1. **Check Device Availability**:\n",
    "   - Check if an XPU is available and set the device accordingly. If the XPU is available and `USE_CPU` is not set to `True`, use the XPU; otherwise, use the CPU.\n",
    "\n",
    "2. **Specify Model Name**:\n",
    "   - Define the model name to be used.\n",
    "\n",
    "3. **Download Model if Not Existing Locally**:\n",
    "   - Define a function to check if the model exists locally.\n",
    "   - If the model does not exist locally, download it from the specified model name, save the tokenizer and model locally.\n",
    "\n",
    "4. **Load Model and Tokenizer**:\n",
    "   - Load the model and tokenizer from the local directory where they were saved.\n",
    "   - Set the padding token and padding side for the tokenizer.\n",
    "   - Resize the model's embeddings to account for any new special tokens added.\n",
    "   - Set the padding token ID in the model's generation configuration.\n",
    "\n",
    "5. **Move Model to Device**:\n",
    "   - Move the model to the appropriate device (XPU or CPU).\n",
    "\n",
    "6. **Configure Model for Fine-Tuning**:\n",
    "   - Disable the caching mechanism to reduce memory usage during fine-tuning.\n",
    "   - Configure the model's pre-training teigured for use on the appropriate device, preparing them for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fef5d77e-e2b4-4944-9906-655a1b030da4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check if XPU is available and set the device accordingly\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() and not USE_CPU else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Specify the model name\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Define a function to check if the model exists locally\n",
    "def download_model_if_not_exist(model_name):\n",
    "    model_dir = os.path.join(\"models\", model_name)\n",
    "    if not os.path.exists(model_dir):\n",
    "        print(f\"Downloading model {model_name}...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        tokenizer.save_pretrained(model_dir)\n",
    "        model.save_pretrained(model_dir)\n",
    "        print(f\"Model {model_name} downloaded and saved locally.\")\n",
    "    else:\n",
    "        print(f\"Model {model_name} already exists locally.\")\n",
    "    return model_dir\n",
    "\n",
    "# Call the function to download the model if it doesn't exist\n",
    "model_dir = download_model_if_not_exist(model_name)\n",
    "\n",
    "# Load the model and tokenizer from the local directory\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_dir)\n",
    "\n",
    "# Set the padding token and padding side\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Resize the model embeddings to account for the new special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Set the padding token ID for generation configuration\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Configure the model's pre-training tensor parallelism degree\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"Model and tokenizer are ready for use.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38530297-4387-4420-98cc-fd9e9c2ba59a",
   "metadata": {},
   "source": [
    "### Step 8: Testing the Model \n",
    "\n",
    "Before starting the fine-tuning process, let's evaluate the LLaMa3 model on a sample input to observe its initial performance. We'll generate responses for a few questions from the `test_inputs` list belo🌿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd310c-0e10-4d5b-be0e-5baca18028ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def generate_response(model, prompt):\n",
    "    \"\"\"\n",
    "    Generate a response from the model given a prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to use for generating the response.\n",
    "        prompt (str): The input prompt to generate a response for.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response as a string.\n",
    "    \"\"\"\n",
    "    # Tokenize the input prompt and move it to the specified device\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
    "    \n",
    "    # Generate a response from the model\n",
    "    outputs = model.generate(input_ids, max_new_tokens=100, \n",
    "                             eos_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # Decode the generated tokens and return the response as a string\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "def test_model(model, test_inputs):\n",
    "    \"\"\"\n",
    "    Quickly test the model using a set of test queries.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to test.\n",
    "        test_inputs (list of str): A list of input prompts to test the model with.\n",
    "    \"\"\"\n",
    "    # Iterate over each test input\n",
    "    for input_text in test_inputs:\n",
    "        print(\"__\" * 50)\n",
    "        # Generate a response for the input prompt\n",
    "        generated_response = generate_response(model, input_text)\n",
    "        # Print the input prompt and the generated response\n",
    "        print(f\"Input: {input_text}\")\n",
    "        print(f\"Generated Answer: {generated_response}\\n\")\n",
    "        print(\"__\" * 50)\n",
    "\n",
    "# Define a list of test input prompts to evaluate the model\n",
    "test_inputs = [\n",
    "    \"How do I check the status of the RAID array on my DGX system?\",\n",
    "    \"Can you show me how to get detailed information about the RAID configuration on my DGX?\",\n",
    "    \"How can I allow a user to access Docker on the DGX?\"\n",
    "]\n",
    "\n",
    "# Print a message indicating the start of model testing\n",
    "print(\"Testing the model before fine-tuning:\")\n",
    "# Test the model with the defined test inputs\n",
    "test_model(model, test_inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c9c6f2-805d-4ac4-8838-e545b99aa41b",
   "metadata": {},
   "source": [
    "### Step 9: Load and Inspect the Dataset 📊\n",
    "Import the load_dataset function and load the specified dataset from the Hugging Face datasets library. In this case, the dataset identifier is RayBernard/nvidia-dgx-best-practices, and we are loading the training split of the dataset. Print the first instruction and response from the dataset to ensure the content is as expected. Next, print the total number of examples in the dataset to understand its size. List the fields (keys) present in the dataset to understand its structure. Finally, print the entire dataset to get an overview of its structure and contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64993e6-9356-4d98-b5fd-1a913f6c3c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a specific dataset from the Hugging Face datasets library.\n",
    "# 'RayBernard/nvidia-dgx-best-practices' is the identifier for the dataset,\n",
    "# and 'split=\"train\"' specifies that we want the training split of the dataset.\n",
    "dataset_name = \"RayBernard/nvidia-dgx-best-practices\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Print the first instruction and response from the dataset to verify the content.\n",
    "print(f\"Instruction is: {dataset[0]['instruction']}\")\n",
    "print(f\"Response is: {dataset[0]['output']}\")\n",
    "\n",
    "# Print the number of examples in the dataset.\n",
    "print(f\"Number of examples in the dataset: {len(dataset)}\")\n",
    "\n",
    "# Print the fields (keys) present in the dataset.\n",
    "print(f\"Fields in the dataset: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Print the entire dataset to get an overview of its structure and contents.\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b09ad6-7d1c-47a8-b547-15d2bcc8f51b",
   "metadata": {},
   "source": [
    "### Step 10: Format and Split the Dataset for Training\n",
    "\n",
    "This step ensures your dataset is properly formatted and split for the training process, making it ready for fine-tuning.\n",
    "\n",
    "1. **Load and Define**:\n",
    "   - Load the dataset with the specified name and split. Here, we are loading the \"train\" split of the dataset.\n",
    "   - Define the system message to be used for formatting prompts.\n",
    "\n",
    "2. **Format Prompts**:\n",
    "   - Use the `format_prompts` function to format the dataset prompts according to the Meta Llama 3 Instruct prompt template with special tokens.\n",
    "   - This function iterates over the 'instruction' and 'output' fields in the batch and formats them accordingly.\n",
    "   - Apply the `format_prompts` function to the dataset in a batched manner for efficiency.\n",
    "\n",
    "3. **Split the Dataset**:\n",
    "   - Split the formatted dataset into training and validation sets, using 20% of the data for validation and setting a seed for reproducibility.\n",
    "\n",
    "4. **Verify the Split**:\n",
    "   - Print the number of examples in both the training and validation sets to verify the split.\n",
    "\n",
    "5. **Show Formatted Prompt**:\n",
    "   - Define and use a function to show the formatted prompt for the first record, demonstrating what the prompt looks like with the system message included.\n",
    "\n",
    "This process ensures that your dataset is well-organized and ready for the training phase, enhancing the model's performance during fine-tuning.`tuning phase.e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f040aea3-1d9d-43cf-97e1-a927efd85ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset with the specified name and split\n",
    "# Here, we are loading the \"train\" split of the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Define the system message separately\n",
    "system_message = \"You are a helpful  linux configuration  AI, who only responds with commands used to execuite over SSH. you are to think step by step on what they are, since your job depends on it.  Format the to be place in an  ssh session\"\n",
    "\n",
    "def format_prompts(batch, system_msg):\n",
    "    \"\"\"\n",
    "    Format the prompts according to the Meta Llama 3 Instruct prompt template with special tokens.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A batch of data containing 'instruction' and 'output' fields.\n",
    "        system_msg (str): The system message to be included in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the formatted prompts under the 'text' key.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the formatted prompts\n",
    "    formatted_prompts = []\n",
    "\n",
    "    # Iterate over the 'instruction' and 'output' fields in the batch\n",
    "    for instruction, output in zip(batch[\"instruction\"], batch[\"output\"]):\n",
    "        # Format the prompt according to the Meta Llama 3 Instruct template with special tokens\n",
    "        prompt = (\n",
    "            \"<|startoftext|>system\\n\"\n",
    "            f\"{system_msg}\\n\"\n",
    "            \"<|endoftext|>user\\n\"\n",
    "            f\"{instruction}\\n\"\n",
    "            \"<|endoftext|>assistant\\n\"\n",
    "            f\"{output}\\n\"\n",
    "            \"<|endoftext|>\"\n",
    "        )\n",
    "        # Append the formatted prompt to the list\n",
    "        formatted_prompts.append(prompt)\n",
    "\n",
    "    # Return the formatted prompts as a dictionary with the key 'text'\n",
    "    return {\"text\": formatted_prompts}\n",
    "\n",
    "# Apply the format_prompts function to the dataset\n",
    "# The function is applied in a batched manner to speed up processing\n",
    "formatted_dataset = dataset.map(lambda batch: format_prompts(batch, system_message), batched=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "# 20% of the data is used for validation, and a seed is set for reproducibility\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.2, seed=99)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_dataset = split_dataset[\"test\"]\n",
    "print(\"train dataset == \",train_dataset)\n",
    "print(\"validation dataset ==\", validation_dataset)\n",
    "# Print the number of examples in the training and validation sets\n",
    "print(f\"Number of examples in the training set: {len(train_dataset)}\")\n",
    "print(f\"Number of examples in the validation set: {len(validation_dataset)}\")\n",
    "\n",
    "# Function to show what the prompt looks like for the first record with the system message\n",
    "def show_first_prompt(system_msg):\n",
    "    # Get the first record from the dataset\n",
    "    first_instruction = dataset[\"instruction\"][0]\n",
    "    first_output = dataset[\"output\"][0]\n",
    "    \n",
    "    # Format the first record using the provided system message\n",
    "    prompt = (\n",
    "        \"<|startoftext|>system\\n\"\n",
    "        f\"{system_msg}\\n\"\n",
    "        \"<|endoftext|>user\\n\"\n",
    "        f\"{first_instruction}\\n\"\n",
    "        \"<|endoftext|>assistant\\n\"\n",
    "        f\"{first_output}\\n\"\n",
    "        \"<|endoftext|>\"\n",
    "    )\n",
    "    \n",
    "    # Print the original instruction and output\n",
    "    print(f\"Original instruction: {first_instruction}\")\n",
    "    print(f\"Original output: {first_output}\")\n",
    "    \n",
    "    # Print the formatted prompt\n",
    "    print(f\"\\nFormatted prompt with system message:\\n{prompt}\")\n",
    "\n",
    "# Show what the prompt looks like for the first record with the system message\n",
    "show_first_prompt(system_message)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7eba4-7b66-431c-9af4-8b8f6dd3faa5",
   "metadata": {},
   "source": [
    "### Step 11: Fine-Tune the Model and Save the Results\n",
    "\n",
    "1. **Setup Imports and Configurations**:\n",
    "   - Import necessary libraries and modules.\n",
    "   - Check if Intel XPU is available and set the device accordingly.\n",
    "\n",
    "2. **Load Model and Tokenizer**:\n",
    "   - Load the model and tokenizer from the specified path.\n",
    "   - Move the model's embedding layer to the same device and enable gradient for fine-tuning.\n",
    "\n",
    "3. **Set Environment Variables**:\n",
    "   - Configure relevant environment variables for logging and configuration, including Weights and Biases project settings.\n",
    "\n",
    "4. **Load Datasets**:\n",
    "   - Load the training and validation datasets.\n",
    "\n",
    "5. **Configure Training Parameters**:\n",
    "   - Set training parameters including batch size, gradient accumulation steps, learning rate, and mixed precision training.\n",
    "\n",
    "6. **Initialize Trainer**:\n",
    "   - Initialize the `SFTTrainer` with LoRA configuration, including training arguments and datasets.\n",
    "\n",
    "7. **Optimize Performance**:\n",
    "   - Clear the XPU cache before starting the training process.\n",
    "\n",
    "8. **Begin Training**:\n",
    "   - Start the training process.\n",
    "   - Print a summary of the training results, including total training time and samples processed per second.\n",
    "   - Handle any exceptions to ensure smooth execution.\n",
    "\n",
    "9. **Save the Model**:\n",
    "   - Save the fine-tuned LoRA model to the specified path for future use.\n",
    "\n",
    "This step-by-step approach ensures that the model is properly fine-tuned and ready for deployment, with optimal performance configurations and comprehensive logging for tracking progress.e use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd13bc5b-1881-4294-99ce-b7e89afbfb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"IPEX version:\", ipex.__version__)\n",
    "\n",
    "# Check if Intel XPU is available\n",
    "if torch.xpu.is_available():\n",
    "    print(\"Intel XPU is available\")\n",
    "    for i in range(torch.xpu.device_count()):\n",
    "        print(f\"XPU Device {i}: {torch.xpu.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"Intel XPU is not available\")\n",
    "\n",
    "# Set the device to XPU if available, else fallback to CPU\n",
    "device = torch.device(\"xpu:0\" if torch.xpu.is_available() else \"cpu\")\n",
    "\n",
    "# Load model and tokenizer from the specified path\n",
    "model_path = \"Training/AI/GenAI/models/meta-llama/Meta-Llama-3-8B-Instruct\" #Model was download in Step 3\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "model = model.to(device)  # Move the model to the selected device (XPU or CPU)\n",
    "\n",
    "# Move the model's embedding layer to the same device\n",
    "model.embed_tokens = model.get_input_embeddings().to(device)\n",
    "for param in model.embed_tokens.parameters():\n",
    "    param.requires_grad = True  # Enable fine-tuning of word embeddings\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM environment variable to avoid parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set other environment variables for logging and configuration\n",
    "os.environ[\"WANDB_PROJECT\"] = \"llama3-8b-instruct-ft\"  # Weights and Biases project name\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"  # Log model checkpoints to Weights and Biases\n",
    "os.environ[\"IPEX_TILE_AS_DEVICE\"] = \"1\"  # Intel Extension for PyTorch setting for optimal performance\n",
    "\n",
    "# Configuration variables\n",
    "finetuned_model_id = \"RayBernard/llama3-8b-instruct-ft\"  # Model ID for the fine-tuned model\n",
    "PUSH_TO_HUB = True  # Whether to push the model to the Hugging Face Hub\n",
    "USE_WANDB = True  # Whether to use Weights and Biases for logging\n",
    "\n",
    "# Load datasets (assuming these are pre-defined)\n",
    "train_dataset = load_dataset('train_dataset')  # Path to the training dataset\n",
    "validation_dataset = load_dataset('validation_dataset')  # Path to the validation dataset\n",
    "\n",
    "# Training configuration\n",
    "num_train_samples = len(train_dataset)  # Number of training samples\n",
    "batch_size = 2  # Per device batch size, reduced to fit memory\n",
    "gradient_accumulation_steps = 8  # Accumulate gradients over 8 steps to simulate a larger batch size\n",
    "steps_per_epoch = num_train_samples // (batch_size * gradient_accumulation_steps)  # Steps per epoch\n",
    "num_epochs = 16  # Number of training epochs\n",
    "max_steps = steps_per_epoch * num_epochs  # Total number of training steps\n",
    "print(f\"Finetuning for max number of steps: {max_steps}\")\n",
    "\n",
    "def print_training_summary(results):\n",
    "    print(f\"Time: {results.metrics['train_runtime']: .2f}\")  # Print total training time\n",
    "    print(f\"Samples/second: {results.metrics['train_samples_per_second']: .2f}\")  # Print training speed\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=batch_size,  # Batch size per device (GPU or XPU)\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,  # Gradient accumulation steps to save memory\n",
    "    warmup_ratio=0.05,  # Ratio of total steps for learning rate warmup to stabilize training\n",
    "    max_steps=max_steps,  # Total number of training steps calculated from epochs and batch size\n",
    "    learning_rate=3e-5,  # Learning rate for the optimizer\n",
    "    evaluation_strategy=\"steps\",  # Evaluation strategy to evaluate the model at regular steps\n",
    "    save_steps=100,  # Frequency (in steps) to save model checkpoints\n",
    "    fp16=True,  # Enable mixed precision training (16-bit floating point numbers) to save memory\n",
    "    logging_steps=100,  # Frequency (in steps) to log training metrics\n",
    "    output_dir=finetuned_model_id,  # Directory to save the model and training outputs\n",
    "    hub_model_id=finetuned_model_id if PUSH_TO_HUB else None,  # Model ID for pushing to Hugging Face Hub\n",
    "    report_to=\"wandb\" if USE_WANDB else None,  # Reporting to Weights and Biases for experiment tracking\n",
    "    push_to_hub=PUSH_TO_HUB,  # Whether to push the model to the Hugging Face Hub\n",
    "    max_grad_norm=0.6,  # Max gradient norm for gradient clipping to prevent exploding gradients\n",
    "    weight_decay=0.01,  # Weight decay for regularization to prevent overfitting\n",
    "    group_by_length=True,  # Group sequences by length to improve training efficiency\n",
    "    gradient_checkpointing=True  # Enable gradient checkpointing to save memory by trading compute\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer with LoRA configuration\n",
    "lora_config = LoraConfig()  # LoRA configuration (assumed to be defined)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,  # Model to train\n",
    "    train_dataset=train_dataset,  # Training dataset\n",
    "    eval_dataset=validation_dataset,  # Validation dataset\n",
    "    tokenizer=tokenizer,  # Tokenizer\n",
    "    args=training_args,  # Training arguments\n",
    "    peft_config=lora_config,  # LoRA configuration\n",
    "    dataset_text_field=\"text\",  # Field name in the dataset containing the text data\n",
    "    max_seq_length=512,  # Maximum sequence length for training\n",
    "    packing=True  # Enable sequence packing for efficiency\n",
    ")\n",
    "\n",
    "try:\n",
    "    # Clear XPU cache before starting the training\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    # Start training\n",
    "    results = trainer.train()\n",
    "    \n",
    "    # Print training summary\n",
    "    print_training_summary(results)\n",
    "    wandb.finish()  # Finish the wandb run\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")  # Print any errors that occur during training\n",
    "\n",
    "# Save the fine-tuned LoRA model\n",
    "tuned_lora_model = \"llama3-8b-instruct-ftuned\"\n",
    "trainer.model.save_pretrained(tuned_lora_model)  # Save the trained model to the specified path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86f1be9-87dc-4b1c-848d-eb1190bdb194",
   "metadata": {},
   "source": [
    "### Step 12: Merge and Save the Fine-Tuned Model\n",
    "\n",
    "After fine-tuning the model, merge the fine-tuned LoRA model with the base model and save the final tuned model. This process ensures that the fine-tuning adjustments are integrated into the base model, resulting in an optimized and ready-to-use model.\n",
    "\n",
    "1. **Import Required Libraries**: Import the necessary libraries from `peft` and `transformers`.\n",
    "2. **Load Base Model**: Load the base model using `AutoModelForCausalLM` with the specified model ID and configurations to optimize memory usage and performance.\n",
    "3. **Merge Models**: Use `PeftModel` to load the fine-tuned LoRA model and merge it with the base model.\n",
    "4. **Unload Unnecessary Parameters**: Merge and unload unnecessary parameters from the model to optimize it.\n",
    "5. **Save the Final Model**: Save the final merged model to the specified path for future use.\n",
    "\n",
    "This step finalizes the training process by producing a single, fine-tuned model ready for del.save_pretrained(tunmodel)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b981a9-e0f7-4f7d-bc59-a3f54f51a772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments\n",
    "\n",
    "tuned_model = \"RayBernard/llama3-8b-instruct-ft-dgx\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "model = PeftModel.from_pretrained(base_model, tuned_lora_model)\n",
    "model = model.merge_and_unload()\n",
    "# save final tuned model\n",
    "model.save_pretrained(tuned_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a7e6b4-6d0e-4306-9682-227559d49b52",
   "metadata": {},
   "source": [
    "### Step 13: Upload the Fine-Tuned Model to Hugging Face Hub 🚀\n",
    "\n",
    "1. **Install Necessary Libraries**:\n",
    "   - Ensure you have the `huggingface_hub` library installed.\n",
    "\n",
    "2. **Import Libraries**:\n",
    "   - Import the necessary modules for interacting with the Hugging Face Hub.\n",
    "\n",
    "3. **Authenticate with Hugging Face Hub**:\n",
    "   - Set your Hugging Face token as an environment variable.\n",
    "   - Log in to Hugging Face Hub using the token.\n",
    "\n",
    "4. **Define the Path and Repository**:\n",
    "   - Specify the path to your fine-tuned model.\n",
    "   - Define the name of the repository you want to create on Hugging Face Hub.\n",
    "\n",
    "5. **Upload Files to Hugging Face Hub**:\n",
    "   - Use the `HfApi` class to create the repository if it doesn't already exist.\n",
    "   - Upload all files from the specified path to the repository.\n",
    "\n",
    "This step ensures your fine-tuned model is uploaded to the Hugging Face Hub, making it accessible for future use and sharing with the community. This process uploads your fine-tuned model to the Hugging Face Hub, making it available for easy access and sharing.ss and sharing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcb0395-94c9-4a48-901a-bfbb64201bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Necessary Libraries\n",
    "!pip install huggingface_hub\n",
    "\n",
    "# Step 2: Import Libraries\n",
    "import os\n",
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Step 3: Authenticate with Hugging Face Hub\n",
    "# Make sure to set your Hugging Face token in the environment variable\n",
    "os.environ['HUGGINGFACE_TOKEN'] = \"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\"\n",
    "\n",
    "# Login to Hugging Face Hub\n",
    "login(token=os.getenv('HUGGINGFACE_TOKEN'))\n",
    "\n",
    "# Step 4: Define the Path and Repository\n",
    "model_path = \"RayBernard/llama3-8b-instruct-ft-dgx\"\n",
    "\n",
    "# Name of the repo you want to create on huggingface \n",
    "repository_name = \"RayBernard/llama3-8b-instruct-ft-dgx\"\n",
    "\n",
    "# Step 5: Upload Files to Hugging Face Hub\n",
    "api = HfApi()\n",
    "\n",
    "# Create the repository if it doesn't exist\n",
    "api.create_repo(repo_id=repository_name, exist_ok=True)\n",
    "\n",
    "# Upload all files from the specified path to the repository\n",
    "for root, _, files in os.walk(model_path):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        api.upload_file(\n",
    "            path_or_fileobj=file_path,\n",
    "            path_in_repo=os.path.relpath(file_path, model_path),\n",
    "            repo_id=repository_name\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4523b5b8-8c3f-40d6-8095-e15173d35538",
   "metadata": {},
   "source": [
    "### Step 14: Fine-Tuning Results and Observations\n",
    "\n",
    "After fine-tuning the LLaMA3 model oourse question-answering dataset, we observed significant improvements in the model's ability to provide accurate and relevant responses to a wide range of queries. The fine-tuned model demonstrated a better understanding of domain-specific terminology and concepts compared to the baseline model.\n",
    "\n",
    "The model's performance was evaluated on a held-out test set, achieving promising results in terms of accuracy and coherence. The fine-tuned model was able to generate more contextually appropriate and informative responses compared to the generic model.\n",
    "\n",
    "However, it is important to note that the model's performance may still be limited by the size and diversity of the fine-tuning dataset. Expanding the dataset with more varied questions and answers across different domains could further enhance the model's capabilities and generalization.\n",
    "\n",
    "Overall, the fine-tuned model shows promise in assisting users with their information needs across various topics, but it should be used as a complementary tool alongside other reliable sources of information.rmation.rmation.rmation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41572a4-3d83-49e4-9827-94ae78fabdc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define the local path to the model\n",
    "local_model_path = \"llama3-8b-instruct-ft-dgx\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(local_model_path)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the model\n",
    "model = AutoModelForCausalLM.from_pretrained(local_model_path)\n",
    "\n",
    "# Load the PEFT fine-tuned model, if applicable\n",
    "model = PeftModel.from_pretrained(model, local_model_path)\n",
    "\n",
    "# Move the model to the correct device\n",
    "device = torch.device(\"xpu:0\" if torch.xpu.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "# Test inputs\n",
    "test_inputs = [\n",
    "       \"How do I check the status of the RAID array on my DGX system?\",\n",
    "       \"Can you show me how to get detailed information about the RAID configuration on my DGX?\",\n",
    "       \"How can I allow a user to access Docker on the DGX?\"\n",
    "]\n",
    "\n",
    "# Run inference on test inputs\n",
    "for text in test_inputs:\n",
    "    # Tokenize the input text and convert to PyTorch tensors, then move to the selected device (XPU or CPU)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate text based on the input, with the following parameters:\n",
    "    outputs = model.generate(\n",
    "        **inputs,  # Pass the tokenized inputs to the model\n",
    "        max_new_tokens=100,  # Maximum number of new tokens to generate\n",
    "        do_sample=True,  # Use sampling for generation (as opposed to greedy decoding)\n",
    "        top_k=100,  # Use top-k sampling, considering the top 100 tokens\n",
    "        temperature=0.9,  # Sampling temperature; higher values mean more randomness\n",
    "        eos_token_id=tokenizer.eos_token_id  # End-of-sequence token ID to stop generation\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens back to text and print the result, skipping special tokens\n",
    "    print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15de8308-1530-44ed-838c-d1db36095837",
   "metadata": {},
   "source": [
    "### Happy Fine-Tuning! 😄✨\n",
    "\n",
    "Congratulations on reaching this milestone! You now have the tools and knowledge to fine-tune the powerful LLaMA3 model on your own datasets. Feel free to experiment, customize, and adapt this notebook to fit your specific use case. Try different datasets, tweak the hyperparameters, and observe how the model's performance evolves.\n",
    "\n",
    "We encourage you to share your fine-tuned models and experiences with the community. Consider open-sourcing your work on platforms like GitHub or Hugging Face, and write blog posts to detail your journey. Your insights and achievements can inspire and assist others in their own fine-tuning projects.\n",
    "\n",
    "If you encounter any issues or have suggestions for improvement, please don't hesitate to reach out and provide feedback. We value your input and are committed to making this notebook and the fine-tuning process as smooth and enjoyable as possible."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Modin",
   "language": "python",
   "name": "modin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
