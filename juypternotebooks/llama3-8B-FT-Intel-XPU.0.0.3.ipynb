{
 "cells": [
  {
   "cell_type": "raw",
   "id": "8f15b0f9-3b7b-42cb-81fe-4ada30017540",
   "metadata": {},
   "source": [
    "SPDX-License-Identifier: Apache-2.0 \n",
    "Copyright (c) 2024, Rahul Unnikrishnan Nair <rahul.unnikrishnan.nair@intel.com>\n",
    "Ray Bernard <ray.bernard@outlookcom>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473516ba-176f-4a44-9f28-f74dab5d2dd4",
   "metadata": {},
   "source": [
    "## Fine-Tuning LLaMa 3 8B Instruct on Intel Max Series GPUs: An Exciting Journey\r\n",
    "\r\n",
    "In this guide, we embark on an exciting journey to fine-tune the powerful LLaMa 3 8B Instruct model using a custom dataset on Intel Max Series GPUs. Intel GPUs offer incredible speed and cost-effectiveness, making them an attractive choice for training large language models.\r\n",
    "\r\n",
    "I successfully trained the LLaMa 3 8B Instruct model using my custom dataset, leveraging the power of HuggingFace. By importing my dataset, obtaining the LLaMa 3 8B Instruct model, and training it using the Transformer Trainer, I was able to achieve outstanding results.\r\n",
    "\r\n",
    "The entire training process was meticulously monitored by Weights and Biases, providing detailed insights into memory usage, disk I/O, training loss, and more. It's truly an outstanding product!\r\n",
    "\r\n",
    "The best part? It's all free! I was amazed by the capabilities of the Intel Developer Cloud, particularly for machine learning (ML), high-performance computing (HPC), and generative AI (GenAI) workflows. The IntelÂ® Data Center Max 1100 GPU, a high-performance 300-watt double-wide AIC card, features 56 Xe cores and 48 GB of HBM2E memory, delivering exceptional performance.\r\n",
    "\r\n",
    "In the spirit of open source, I developed the following notebook, partially based on the original work of Rahul Unnikrishnan Nair from Intel, \"Fine-tuning Google's Gemma Model on Intel Max Series GPUs.\" Thank you for the foundation. I was able to significantly tweak and create a process to fine-tune LLaMa 3 models, which represent a significant leap from LLaMa 2 in terms of capabilities. It's amazing how such small models can produce great results with quality data.\r\n",
    "\r\n",
    "I will be producing a detailed video review of the notebook, which you can find on my YouTube channel: [YouTube Channel](https://www.youtube.com/channel/UC-OszhqWsF1tqqECdeLI_7Q).\r\n",
    "\r\n",
    "**Note: This code was executed in a Jupyter notebook on Intel's Developer Cloud.**\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d278bc-2b59-4cf3-8d2c-8a800afc84e1",
   "metadata": {},
   "source": [
    "### Set Path if you receive a path error while installing software\n",
    "The below sets the path for your .local/bin . \n",
    "The root directory needs to be change to  your root directory.  '/home/<userrid--change me>/.local/bin'\n",
    "You can find out your userid by launching a terminal executing at the prompt $ pwd  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c1ca4-9896-45f8-9ce6-8446dc29d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "user_dir = '/home/u2b3e96b2fc320ef8c781f51df67225d/'\n",
    "# Add the directory to the PATH\n",
    "os.environ['PATH'] += os.pathsep + user_dir + '.local/bin'\n",
    "\n",
    "# Verify the PATH update\n",
    "print(\"Updated PATH:\", os.environ['PATH'])\n",
    "\n",
    "# Check if the directory is now in PATH\n",
    "if user_dir + '.local/bin' in os.environ['PATH']:\n",
    "    print(\"Directory successfully added to PATH.\")\n",
    "else:\n",
    "    print(\"Failed to add directory to PATH.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487cea3-9396-4ed6-9363-5452144fb522",
   "metadata": {},
   "source": [
    "### Run only once to make sure you have the proper versions of the base software needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7038e25-e2fa-44df-96d8-558b39fc36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install torch==2.1.0.post2 torchvision==0.16.0.post2 torchaudio==2.1.0.post2 intel-extension-for-pytorch==2.1.30.post0 oneccl_bind_pt==2.1.300+xpu --extra-index-url https://pytorch-extension.intel.com/release-whl/stable/xpu/us/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98609b8-42d5-4aa7-87e4-95e54639279b",
   "metadata": {},
   "source": [
    "### Step 1: Initial Setup\r\n",
    "\r\n",
    "Before we begin the fine-tuning process, it's essential to ensure that we have the proper libraries installed and the correct kernel configured. This step needs to be performed only once.\r\n",
    "First, make sure you are using the Modin kernel on the Intel Developer Cloud. The Modin kernel is a specialized kernel designed for efficient data processing and analysis. To access the Modin kernel, follow these steps:\r\n",
    "Join the Intel Developer Cloud by creating an account.\r\n",
    "Once logged in, navigate to the \"Free Training\" section.\r\n",
    "You will be presented with a Jupyter Lab environment, where you can select the Modin kernel.Run only once to make sure you have the proper versions of the additional software needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc04dd16-1f72-45d3-af53-ad811ab13e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import site\n",
    "import os\n",
    "\n",
    "# Install the required packages\n",
    "!{sys.executable} -m pip install --upgrade \"transformers>=4.38.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"datasets>=2.18.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"wandb>=0.17.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"trl>=0.7.11\"\n",
    "!{sys.executable} -m pip install --upgrade \"peft>=0.9.0\"\n",
    "!{sys.executable} -m pip install --upgrade \"accelerate>=0.28.*\"\n",
    "!{sys.executable} -m pip install --upgrade \"huggingface_hub\"\n",
    "\n",
    "# Get the site-packages directory\n",
    "site_packages_dir = site.getsitepackages()[0]\n",
    "\n",
    "# add the site pkg directory where these pkgs are insalled to the top of sys.path\n",
    "if not os.access(site_packages_dir, os.W_OK):\n",
    "    user_site_packages_dir = site.getusersitepackages()\n",
    "    if user_site_packages_dir in sys.path:\n",
    "        sys.path.remove(user_site_packages_dir)\n",
    "    sys.path.insert(0, user_site_packages_dir)\n",
    "else:\n",
    "    if site_packages_dir in sys.path:\n",
    "        sys.path.remove(site_packages_dir)\n",
    "    sys.path.insert(0, site_packages_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "503dc53f-02b3-4cbc-88c3-a33add5bca1b",
   "metadata": {},
   "source": [
    "## Optionally, Check to see if have installed versions of the base software"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397078f5-4145-4341-beda-b554b3b9175e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "import trl\n",
    "import peft\n",
    "import datasets\n",
    "\n",
    "# Get versions of the libraries\n",
    "torch_version = torch.__version__\n",
    "transformers_version = transformers.__version__\n",
    "wandb_version = wandb.__version__\n",
    "trl_version = trl.__version__\n",
    "peft_version = peft.__version__\n",
    "datasets_version = datasets.__version__\n",
    "\n",
    "# Print the versions\n",
    "print(f\"torch version: {torch_version}\")\n",
    "print(f\"transformers version: {transformers_version}\")\n",
    "print(f\"wandb version: {wandb_version}\")\n",
    "print(f\"trl version: {trl_version}\")\n",
    "print(f\"peft version: {peft_version}\")\n",
    "print(f\"datasets version: {datasets_version}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf73dc5-f681-4f23-95ad-27b0e41b4ee4",
   "metadata": {},
   "source": [
    "### Step 2: Check Intel XPU Availability and Retrieve Device Capabilities\n",
    "In this step, we will import necessary libraries, check the availability of Intel XPU (eXtreme Performance Unit), and retrieve detailed device capabilities. \n",
    "This ensures that our environment is correctly configured to leverage the Intel XPU optimal performance.\n",
    "\n",
    "\n",
    "To optimize performance when using Intel Max Series GPUs:\n",
    "\n",
    "1. **Retrieve CPU Information**: Determine the number of physical CPU cores and calculate cores per socket using `psutil`.\n",
    "2. **Set Environment Variables**:\n",
    "   - Disable tokenizers parallelism.\n",
    "   - Improve memory allocation with `LD_PRELOAD` (optional).\n",
    "   - Reduce GPU command submission overhead.\n",
    "   - Enable SDP fusion for efficient memory usage.\n",
    "   - Configure OpenMP to use physical cores, bind threads, and set thread pinning.\n",
    "3. **Print Configuration**: Display the number of physical cores, cores per socket, and OpenMP environment variables to verify the settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577cc356-6cec-411e-b258-22d0e7eb9202",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import intel_extension_for_pytorch as ipex\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# Check if Intel XPU is available\n",
    "if torch.xpu.is_available():\n",
    "    print(\"Intel XPU is available\")\n",
    "    for i in range(torch.xpu.device_count()):\n",
    "        print(f\"XPU Device {i}: {torch.xpu.get_device_name(i)}\")\n",
    "    \n",
    "    # Get the device capability details\n",
    "    device_capability = torch.xpu.get_device_capability()\n",
    "    \n",
    "    # Convert the device capability details to a JSON string with indentation for readability\n",
    "    readable_device_capability = json.dumps(device_capability, indent=4)\n",
    "    \n",
    "    # Print the readable JSON\n",
    "    print(\"Detail of GPU capability =\\n\", readable_device_capability)\n",
    "else:\n",
    "    print(\"Intel XPU is not available\")\n",
    "\n",
    "\n",
    "num_physical_cores = psutil.cpu_count(logical=False)\n",
    "num_cores_per_socket = num_physical_cores // 2\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "#HF_TOKEN = os.environ[\"HF_TOKEN\"]\n",
    "\n",
    "# Set the LD_PRELOAD environment variable\n",
    "ld_preload = os.environ.get(\"LD_PRELOAD\", \"\")\n",
    "conda_prefix = os.environ.get(\"CONDA_PREFIX\", \"\")\n",
    "# Improve memory allocation performance, if tcmalloc is not availab>?le, please comment this line out\n",
    "os.environ[\"LD_PRELOAD\"] = f\"{ld_preload}:{conda_prefix}/lib/libtcmalloc.so\"\n",
    "# Reduce the overhead of submitting commands to the GPU\n",
    "os.environ[\"SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS\"] = \"1\"\n",
    "# reducing memory accesses by fusing SDP ops\n",
    "os.environ[\"ENABLE_SDP_FUSION\"] = \"1\"\n",
    "# set openMP threads to number of physical cores\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(num_physical_cores)\n",
    "# Set the thread affinity policy\n",
    "os.environ[\"OMP_PROC_BIND\"] = \"close\"\n",
    "# Set the places for thread pinning\n",
    "os.environ[\"OMP_PLACES\"] = \"cores\"\n",
    "\n",
    "print(f\"Number of physical cores: {num_physical_cores}\")\n",
    "print(f\"Number of cores per socket: {num_cores_per_socket}\")\n",
    "print(f\"OpenMP environment variables:\")\n",
    "print(f\"  - OMP_NUM_THREADS: {os.environ['OMP_NUM_THREADS']}\")\n",
    "print(f\"  - OMP_PROC_BIND: {os.environ['OMP_PROC_BIND']}\")\n",
    "print(f\"  - OMP_PLACES: {os.environ['OMP_PLACES']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be553cb-eb1e-44d1-b144-8c84ca926ace",
   "metadata": {},
   "source": [
    "### Step 2: Monitor XPU Memory Usage in Real-Time\n",
    "\n",
    "The following script sets up a real-time monitoring system that continuously displays the XPU memory usage in a Jupyter notebook, helping you keep track of resource utilization during model training and inference. This setup helps in maintaining optimal performance and preventing resource-related issues during your deep learning tasks.  By keeping track of memory usage, you can prevent out-of-memory errors, optimize resource allocation, and ensure smooth training and inference processes. By monitoring these metrics, you can predict out-of-memory issues. If memory usage approaches the hardware limits, itâs an indication that the model or batch size might need adjusted etc.\n",
    "   - **Memory Reserved**: Indicates the total memory reserved by the XPU. Helps in understanding the memory footprint of the running processes.\n",
    "   - **Memory Allocated**: Shows the actual memory usage by tensors, crucial for identifying memory leaks or excessive usage.\n",
    "   - **Max Memory Reserved/Allocated**: These metrics help in identifying peak memory usage, which is essential for planning and scaling your models.\n",
    "   - performance and preventing resource-related issues during your deep learning tasks.eemory_monitor(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf42a54e-5566-48ea-9c6f-66473fd94eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p>XPU (Intel(R) Data Center GPU Max 1100) :: Memory: Reserved=0.0 GB, Allocated=0.0 GB, Max Reserved=0.0 GB, Max Allocated=0.0 GB</p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import psutil\n",
    "import torch\n",
    "import json \n",
    "import asyncio\n",
    "import threading\n",
    "from IPython.display import display, HTML\n",
    "import intel_extension_for_pytorch as ipex\n",
    "\n",
    "if torch.xpu.is_available():\n",
    "    torch.xpu.empty_cache()\n",
    "    \n",
    "    def get_memory_usage():\n",
    "        memory_reserved = round(torch.xpu.memory_reserved() / 1024**3, 3)\n",
    "        memory_allocated = round(torch.xpu.memory_allocated() / 1024**3, 3)\n",
    "        max_memory_reserved = round(torch.xpu.max_memory_reserved() / 1024**3, 3)\n",
    "        max_memory_allocated = round(torch.xpu.max_memory_allocated() / 1024**3, 3)\n",
    "        return memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated\n",
    "   \n",
    "    def print_memory_usage():\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        print(f\"XPU Name: {device_name}\")\n",
    "        memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "        memory_usage_text = f\"XPU Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "        print(f\"\\r{memory_usage_text}\", end=\"\", flush=True)\n",
    "    \n",
    "    async def display_memory_usage(output):\n",
    "        device_name = torch.xpu.get_device_name()\n",
    "        output.update(HTML(f\"<p>XPU Name: {device_name}</p>\"))\n",
    "        while True:\n",
    "            memory_reserved, memory_allocated, max_memory_reserved, max_memory_allocated = get_memory_usage()\n",
    "            memory_usage_text = f\"XPU ({device_name}) :: Memory: Reserved={memory_reserved} GB, Allocated={memory_allocated} GB, Max Reserved={max_memory_reserved} GB, Max Allocated={max_memory_allocated} GB\"\n",
    "            output.update(HTML(f\"<p>{memory_usage_text}</p>\"))\n",
    "            await asyncio.sleep(5)\n",
    "    \n",
    "    def start_memory_monitor(output):\n",
    "        loop = asyncio.new_event_loop()\n",
    "        asyncio.set_event_loop(loop)\n",
    "        loop.create_task(display_memory_usage(output))\n",
    "        thread = threading.Thread(target=loop.run_forever)\n",
    "        thread.start()    \n",
    "    output = display(display_id=True)\n",
    "    start_memory_monitor(output)\n",
    "else:\n",
    "    print(\"XPU device not available.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e86421e-64a8-4a05-84f3-fcc0dd80afc8",
   "metadata": {},
   "source": [
    "### Step 3: Load and Prepare the Model\n",
    "\n",
    "In this step, we ensure the model is loaded and prepared for use on the appropriate device, either an Intel XPU or CPU, and configure it for efficient fine-tuningThis ensures the model and tokenizer are properly set up and optimized for use on the selected device, ready for efficient fine-tuning.\n",
    "This step ensures that the model and tokenizer are correctly set up and configured for use on the appropriate device, preparing them for the fine-tuning process.\n",
    ".\n",
    "\n",
    "1. **Check Device Availability**:\n",
    "   - Check if an XPU is available and set the device accordingly. If the XPU is available and `USE_CPU` is not set to `True`, use the XPU; otherwise, use the CPU.\n",
    "\n",
    "2. **Specify Model Name**:\n",
    "   - Define the model name to be used.\n",
    "\n",
    "3. **Download Model if Not Existing Locally**:\n",
    "   - Define a function to check if the model exists locally.\n",
    "   - If the model does not exist locally, download it from the specified model name, save the tokenizer and model locally.\n",
    "\n",
    "4. **Load Model and Tokenizer**:\n",
    "   - Load the model and tokenizer from the local directory where they were saved.\n",
    "   - Set the padding token and padding side for the tokenizer.\n",
    "   - Resize the model's embeddings to account for any new special tokens added.\n",
    "   - Set the padding token ID in the model's generation configuration.\n",
    "\n",
    "5. **Move Model to Device**:\n",
    "   - Move the model to the appropriate device (XPU or CPU).\n",
    "\n",
    "6. **Configure Model for Fine-Tuning**:\n",
    "   - Disable the caching mechanism to reduce memory usage during fine-tuning.\n",
    "   - Configure the model's pre-training teigured for use on the appropriate device, preparing them for the fine-tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ce5a0c-9432-4b3a-be60-02cad1d3a5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Check if XPU is available and set the device accordingly\n",
    "USE_CPU = False\n",
    "device = \"xpu:0\" if torch.xpu.is_available() and not USE_CPU else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Specify the model name\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "\n",
    "# Load the model and tokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "# Check if the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Adding padding token to tokenizer.\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    print(f\"Padding token already exists: {tokenizer.pad_token}\")\n",
    "\n",
    "# # Set the padding token and padding side\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# # # Resize the model embeddings to account for the new special tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # Debugging statements\n",
    "print(f\"Padding token: {tokenizer.pad_token}\")\n",
    "print(f\"Padding token ID: {tokenizer.pad_token_id}\")\n",
    "\n",
    "# Set the padding token ID for the generation configuration\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Move the model to the appropriate device\n",
    "model.to(device)\n",
    "\n",
    "# Disable caching mechanism to reduce memory usage during fine-tuning\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Configure the model's pre-training tensor parallelism degree\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "print(\"Model and tokenizer are ready for use.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37123fa-63da-44a2-babf-9be3e7e80d42",
   "metadata": {},
   "source": [
    "## Step 4 Log into your hugging face account with your access token.  \n",
    "Uncheck the Add token as git credential! ðï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af72a5-9b23-4fe5-8ca8-bf45c7bdf7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loggin to huggnigface\n",
    "from huggingface_hub import notebook_login\n",
    "\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27b4bfe-298f-4341-a9fc-5d962227517d",
   "metadata": {},
   "source": [
    "### Step 5: Load and Inspect the Dataset ð\n",
    "Import the load_dataset function and load the specified dataset from the Hugging Face datasets library. In this case, the dataset identifier is RayBernard/nvidia-dgx-best-practices, \n",
    "and we are loading the training split of the dataset. Print the first instruction and response from the dataset to ensure the content is as expected. \n",
    "Next, print the total number of examples in the dataset to understand its size. List the fields (keys) present in the dataset to understand its structure. \n",
    "\n",
    "\n",
    "Format and Split the Dataset for Training\n",
    "\n",
    "This step ensures your dataset is properly formatted and split for the training process, making it ready for fine-tuning.\n",
    "\n",
    "1. **Load and Define**:\n",
    "   - Load the dataset with the specified name and split. Here, we are loading the \"train\" split of the dataset.\n",
    "   - Define the system message to be used for formatting prompts.\n",
    "\n",
    "2. **Format Prompts**:\n",
    "   - Use the `format_prompts` function to format the dataset prompts according to the Meta Llama 3 Instruct prompt template with special tokens.\n",
    "   - This function iterates over the 'instruction' and 'output' fields in the batch and formats them accordingly.\n",
    "   - Apply the `format_prompts` function to the dataset in a batched manner for efficiency.\n",
    "\n",
    "3. **Split the Dataset**:\n",
    "   - Split the formatted dataset into training and validation sets, using 20% of the data for validation and setting a seed for reproducibility.\n",
    "\n",
    "4. **Verify the Split**:\n",
    "   - Print the number of examples in both the training and validation sets to verify the split.\n",
    "\n",
    "5. **Show Formatted Prompt**:\n",
    "   - Define and use a function to show the formatted prompt for the first record, demonstrating what the prompt looks like with the system message included.\n",
    "\n",
    "This process ensures that your dataset is well-organized and ready for the training phase, enhancing the model's performance during fine-tuning.d contents.\r\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e46441-5e45-40cd-87f7-1b9ef5acaeee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load a specific dataset from the Hugging Face datasets library.\n",
    "# 'RayBernard/nvidia-dgx-best-practices' is the identifier for the dataset,\n",
    "# and 'split=\"train\"' specifies that we want the training split of the dataset.\n",
    "dataset_name = \"RayBernard/nvidia-dgx-best-practices\"\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Print the first instruction and response from the dataset to verify the content.\n",
    "print(f\"Instruction is: {dataset[0]['instruction']}\")\n",
    "print(f\"Response is: {dataset[0]['output']}\")\n",
    "\n",
    "\n",
    "# Print the number of examples in the dataset.\n",
    "print(f\"Number of examples in the dataset: {len(dataset)}\")\n",
    "\n",
    "# Print the fields (keys) present in the dataset.\n",
    "print(f\"Fields in the dataset: {list(dataset.features.keys())}\")\n",
    "\n",
    "# Print the entire dataset to get an overview of its structure and contents.\n",
    "print(dataset)\n",
    "\n",
    "# Load the dataset with the specified name and split\n",
    "# Here, we are loading the \"train\" split of the dataset\n",
    "dataset = load_dataset(dataset_name, split=\"train\")\n",
    "\n",
    "# Define the system message separately\n",
    "# system_message = \"Respond with the appropriate command only\"\n",
    "system_message = \"You are a helpful AI \"\n",
    "\n",
    "def format_prompts(batch, system_msg):\n",
    "    \"\"\"\n",
    "    Format the prompts according to the Meta Llama 3 Instruct prompt template with special tokens.\n",
    "\n",
    "    Args:\n",
    "        batch (dict): A batch of data containing 'instruction' and 'output' fields.\n",
    "        system_msg (str): The system message to be included in the prompt.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the formatted prompts under the 'text' key.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the formatted prompts\n",
    "    formatted_prompts = []\n",
    "\n",
    "    # Iterate over the 'instruction' and 'output' fields in the batch\n",
    "    for instruction, output in zip(batch[\"instruction\"], batch[\"output\"]):\n",
    "        # Format the prompt according to the Meta Llama 3 Instruct template with special tokens\n",
    "        prompt = (\n",
    "            \"<|startoftext|>system\\n\"\n",
    "            f\"{system_msg}\\n\"\n",
    "            \"<|endoftext|>user\\n\"\n",
    "            f\"{instruction}\\n\"\n",
    "            \"<|endoftext|>assistant\\n\"\n",
    "            f\"{output}\\n\"\n",
    "            \"<|endoftext|>\"\n",
    "        )\n",
    "        # Append the formatted prompt to the list\n",
    "        formatted_prompts.append(prompt)\n",
    "\n",
    "    # Return the formatted prompts as a dictionary with the key 'text'\n",
    "    return {\"text\": formatted_prompts}\n",
    "\n",
    "# Apply the format_prompts function to the dataset\n",
    "# The function is applied in a batched manner to speed up processing\n",
    "formatted_dataset = dataset.map(lambda batch: format_prompts(batch, system_message), batched=True)\n",
    "\n",
    "# Split the dataset into training and validation sets\n",
    "# 20% of the data is used for validation, and a seed is set for reproducibility\n",
    "split_dataset = formatted_dataset.train_test_split(test_size=0.2, seed=99)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "validation_dataset = split_dataset[\"test\"]\n",
    "print(\"train dataset == \",train_dataset)\n",
    "print(\"validation dataset ==\", validation_dataset)\n",
    "# Print the number of examples in the training and validation sets\n",
    "print(f\"Number of examples in the training set: {len(train_dataset)}\")\n",
    "print(f\"Number of examples in the validation set: {len(validation_dataset)}\")\n",
    "\n",
    "# Function to show what the prompt looks like for the first record with the system message\n",
    "def show_first_prompt(system_msg):\n",
    "    # Get the first record from the dataset\n",
    "    first_instruction = dataset[\"instruction\"][0]\n",
    "    first_output = dataset[\"output\"][0]\n",
    "    \n",
    "    # Format the first record using the provided system message\n",
    "    prompt = (\n",
    "        \"<|startoftext|>system\\n\"\n",
    "        f\"{system_msg}\\n\"\n",
    "        \"<|endoftext|>user\\n\"\n",
    "        f\"{first_instruction}\\n\"\n",
    "        \"<|endoftext|>assistant\\n\"\n",
    "        f\"{first_output}\\n\"\n",
    "        \"<|endoftext|>\"\n",
    "    )\n",
    "    \n",
    "    # Print the original instruction and output\n",
    "    print(f\"Original instruction: {first_instruction}\")\n",
    "    print(f\"Original output: {first_output}\")\n",
    "    \n",
    "    # Print the formatted prompt\n",
    "    print(f\"\\nFormatted prompt with system message:\\n{prompt}\")\n",
    "\n",
    "# Show what the prompt looks like for the first record with the system message\n",
    "show_first_prompt(system_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0fb692-695e-47b1-b825-04d4876be221",
   "metadata": {},
   "source": [
    "### Step 6: Fine-Tune the Model and Save the Results\n",
    "\n",
    "1. **Setup Imports and Configurations**:\n",
    " \n",
    "In this step, we configure the LoRA (Low-Rank Adaptation) settings for efficient training of our model. \n",
    "LoRA is a technique that improves the efficiency of training by reducing the number of parameters through low-rank decomposition. Here, we instantiate a LoraConfig object with specific parameters tailored to our training needs.\n",
    "    \n",
    "    Instantiate LoRA Configuration:\n",
    "    - r: Set to 64, this parameter controls the dimension of the low-rank decomposition, balancing model capacity and efficiency.\n",
    "    - lora_alpha: Set to 16, this scaling factor adjusts the output of the low-rank decomposition, influencing the strength of the adaptation.\n",
    "    - lora_dropout: Set to 0.5, this dropout rate applies regularization to the LoRA layers to prevent overfitting. A higher value increases regularization.\n",
    "    - bias: Set to \"none\", indicating no bias is added to the LoRA layers.\n",
    "    - target_modules: Specifies the layers where the low-rank adaptation will be applied. Here, it includes \"q_proj\", \"k_proj\", \"v_proj\", and \"output_proj\".\n",
    "    - task_type: Set to \"CAUSAL_LM\", indicating that this configuration is for a causal language modeling task.\n",
    "    - This configuration optimizes the model's training efficiency and performance by carefully adjusting the parameters and specifying the target modules for low-rank adaptation.\n",
    "\n",
    "3. **Set Environment Variables**:\n",
    "   - Configure relevant environment variables for logging and configuration, including Weights and Biases project settings.\n",
    "\n",
    "4. **Load Datasets**:\n",
    "   - Load the training and validation datasets.\n",
    "\n",
    "5. **Configure Training Parameters**:\n",
    "   - Set training parameters including batch size, gradient accumulation steps, learning rate, and mixed precision training.\n",
    "\n",
    "6. **Initialize Trainer**:\n",
    "   - Initialize the `SFTTrainer` with LoRA configuration, including training arguments and datasets.\n",
    "\n",
    "7. **Optimize Performance**:\n",
    "   - Clear the XPU cache before starting the training process.\n",
    "\n",
    "8. **Begin Training**:\n",
    "   - Start the training process.\n",
    "   - Print a summary of the training results, including total training time and samples processed per second.\n",
    "   - Handle any exceptions to ensure smooth execution.\n",
    "\n",
    "9. **Save the Model**:\n",
    "   - Save the fine-tuned LoRA model to the specified path for future use.\n",
    "\n",
    "This step-by-step approach ensures that the model is properly fine-tuned and ready for deployment, with optimal performance configurations and comprehensive logging for tracking with Weights and Bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43b6d85-fd2e-414d-adf6-9d93e081e3f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from peft import LoraConfig\n",
    "import wandb\n",
    "\n",
    "# Configuration variables\n",
    "PUSH_TO_HUB = True\n",
    "\n",
    "USE_WANDB = True\n",
    "\n",
    "# Unset LD_PRELOAD\n",
    "os.environ.pop('LD_PRELOAD', None)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.3,\n",
    "    bias=\"all\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"output_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Finetuned model id to push to hf\n",
    "finetuned_model_id = \"RayBernard/llama-3-8B-Instruct-ft\"\n",
    "\n",
    "# Set TOKENIZERS_PARALLELISM environment variable to avoid parallelism warning\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "# Set other environment variables for logging and configuration\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'llama3-8B-FT-Intel-XPU.0.0.1.ipynb'  # Change to your notebook name\n",
    "os.environ[\"WANDB_PROJECT\"] = \"llama3-8b-Instruct-ft\"\n",
    "os.environ[\"WANDB_LOG_MODEL\"] = \"checkpoint\"\n",
    "os.environ[\"IPEX_TILE_AS_DEVICE\"] = \"1\"\n",
    "\n",
    "# Training configuration\n",
    "num_train_samples = len(train_dataset)\n",
    "batch_size = 2\n",
    "gradient_accumulation_steps = 16\n",
    "steps_per_epoch = num_train_samples // (batch_size * gradient_accumulation_steps)\n",
    "num_epochs = 25\n",
    "max_steps = steps_per_epoch * num_epochs\n",
    "print(f\"Finetuning for max number of steps: {max_steps}\")\n",
    "\n",
    "def print_training_summary(results):\n",
    "    print(f\"Time: {results.metrics['train_runtime']: .2f}\")\n",
    "    print(f\"Samples/second: {results.metrics['train_samples_per_second']: .2f}\")\n",
    "\n",
    "training_args = SFTConfig(\n",
    "    run_name=\"llama3-8b-finetuning2\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    warmup_ratio=0.1,\n",
    "    max_steps=max_steps,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    fp16=True,\n",
    "    logging_steps=100,\n",
    "    output_dir=finetuned_model_id,\n",
    "    hub_model_id=finetuned_model_id if PUSH_TO_HUB else None,\n",
    "    report_to=\"wandb\" if USE_WANDB else None,\n",
    "    push_to_hub=PUSH_TO_HUB,\n",
    "    max_grad_norm=0.6,\n",
    "    weight_decay=0.01,\n",
    "    group_by_length=True,\n",
    "    gradient_checkpointing=True\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=validation_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=lora_config,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    packing=True\n",
    ")\n",
    "\n",
    "try:\n",
    "    torch.xpu.empty_cache()\n",
    "    results = trainer.train()\n",
    "    print_training_summary(results)\n",
    "    wandb.finish()\n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {e}\")\n",
    "\n",
    "tuned_lora_model = \"llama3-8b-Instruct-ft-lora\"\n",
    "trainer.model.save_pretrained(tuned_lora_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45124a65-b606-4f38-9237-e78b1dd89c7b",
   "metadata": {},
   "source": [
    "### Step 12: Merge and Save the Fine-Tuned Model\n",
    "\n",
    "After fine-tuning the model, merge the fine-tuned LoRA model with the base model and save the final tuned model. This process ensures that the fine-tuning adjustments are integrated into the base model, resulting in an optimized and ready-to-use model.\n",
    "\n",
    "1. **Import Required Libraries**: Import the necessary libraries from `peft` and `transformers`.\n",
    "2. **Load Base Model**: Load the base model using `AutoModelForCausalLM` with the specified model ID and configurations to optimize memory usage and performance.\n",
    "3. **Merge Models**: Use `PeftModel` to load the fine-tuned LoRA model and merge it with the base model.\n",
    "4. **Unload Unnecessary Parameters**: Merge and unload unnecessary parameters from the model to optimize it.\n",
    "5. **Save the Final Model**: Save the final merged model to the specified path for future use.\n",
    "\n",
    "This step finalizes the training process by producing a single, fine-tuned model-ready inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753a7c4-b456-4f41-9a61-e4c1d3af3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch \n",
    "\n",
    "tuned_lora_model = \"llama3-8b-Instruct-ft-lora\"\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# Load the base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# Check if the tokenizer has a padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    print(\"Adding padding token to tokenizer.\")\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "else:\n",
    "    print(f\"Padding token already exists: {tokenizer.pad_token}\")\n",
    "\n",
    "# Set the padding token and padding side\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "# Load the PEFT model\n",
    "model = PeftModel.from_pretrained(base_model, tuned_lora_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save the final tuned model\n",
    "final_model_path = \"final-tuned-model\"  # Replace with your desired path\n",
    "model.save_pretrained(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(\"Final tuned model and tokenizer saved successfully.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece6befc-6334-42f5-9094-e1fe1ca0059c",
   "metadata": {},
   "source": [
    "## Optional Upload your model to hugging face hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a81a748-8d16-4c0e-a1c5-847ae62eafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder, login\n",
    "import os\n",
    "\n",
    "final_model_path = \"final-tuned-model\"  # Path where the model and tokenizer are saved\n",
    "repo_name = \"llama3-8b-Instruct-finetuned\"  # Name of the repository on Hugging Face Hub\n",
    "username = \"RayBernard\"  # Your username\n",
    "\n",
    "# # Log in to your Hugging Face account and get the token\n",
    "# login()\n",
    "\n",
    "# Retrieve the token from the cache\n",
    "with open(os.path.expanduser(\"~/.cache/huggingface/token\"), \"r\") as token_file:\n",
    "    token = token_file.read().strip()\n",
    "\n",
    "# Create a new repository or use an existing one\n",
    "api = HfApi()\n",
    "api.create_repo(repo_id=repo_name, token=token, exist_ok=True)\n",
    "\n",
    "# Upload the entire model directory to the repository\n",
    "upload_folder(\n",
    "    folder_path=final_model_path,\n",
    "    repo_id=f\"{username}/{repo_name}\",\n",
    "    token=token,\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "print(f\"Model and tokenizer uploaded to Hugging Face Hub repository: {repo_name}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de1bd07-f6e6-4069-aed9-3f52af297ef0",
   "metadata": {},
   "source": [
    "## Test Model without fine tunning ** Note at this point you should restart the kernel and clear so resources are freed up "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "646f55d1-cc83-427d-91f8-6817907203d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/u2b3e96b2fc320ef8c781f51df67225d/.local/lib/python3.9/site-packages/accelerate/utils/modeling.py:1384: UserWarning: Current model requires 4096 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e5ee8e0029148319616583ca2ff2282",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '4'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': '4'}\n",
      "{'role': 'assistant', 'content': 'mlx4 or mlx5'}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages_list = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Just respond with the command\"},\n",
    "        {\"role\": \"user\", \"content\": \"how many gpu are in an h100?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Just respond with the command\"},\n",
    "        {\"role\": \"user\", \"content\": \"how many gpu are in an h200?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Just respond with the command\"},\n",
    "        {\"role\": \"user\", \"content\": \"what kind of switch os to run InfiniBand network\"},\n",
    "    ],\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "for messages in messages_list:\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(outputs[0][\"generated_text\"][-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f42464f-a9cf-44cd-9429-1b367b36be8e",
   "metadata": {},
   "source": [
    "## Test Model after fined tuned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a41572a4-3d83-49e4-9827-94ae78fabdc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c10997b3d904e8e9bc5d5743460a7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'assistant', 'content': 'nvidia-smi -L'}\n",
      "{'role': 'assistant', 'content': 'nvidia-smi -L'}\n",
      "{'role': 'assistant', 'content': 'ip link set enp1s0f0 mtu 65520'}\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "\n",
    "model_id = \"final-tuned-model\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "messages_list = [\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Just respond with the command\"},\n",
    "        {\"role\": \"user\", \"content\": \"how many gpu are in an h100?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Just respond with the command\"},\n",
    "        {\"role\": \"user\", \"content\": \"how many gpu are in an h200?\"},\n",
    "    ],\n",
    "    [\n",
    "        {\"role\": \"system\", \"content\": \"Just respond with the command\"},\n",
    "        {\"role\": \"user\", \"content\": \"what kind of switch os to run InfiniBand network\"},\n",
    "    ],\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "for messages in messages_list:\n",
    "    outputs = pipeline(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(outputs[0][\"generated_text\"][-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431601b-35ce-4f48-afd7-f509eac23074",
   "metadata": {},
   "source": [
    "## Happy Fine-Tuning! ðâ¨\n",
    "\n",
    "Congratulations on reaching this exciting milestone! You now possess the tools and knowledge to fine-tune the powerful LLaMA 3 model on your own custom datasets. This achievement opens up a world of possibilities for you to explore and unleash the full potential of this cutting-edge language model.\n",
    "We encourage you to embrace the spirit of experimentation and exploration. Feel free to customize and adapt this notebook to fit your specific use case. Try different datasets, tweak the hyperparameters, and observe how the model's performance evolves. This hands-on experience will deepen your understanding and allow you to tailor the model to your unique requirements.\n",
    "Moreover, we invite you to share your fine-tuned models and experiences with the broader community. Consider open-sourcing your work on platforms like GitHub or Hugging Face, and write blog posts to detail your fine-tuning journey. Your insights and achievements can inspire and assist others who are embarking on their own fine-tuning projects, fostering a collaborative and supportive environment for knowledge sharing.\n",
    "If you encounter any challenges or have suggestions for improvement, please don't hesitate to reach out and provide feedback. We value your input and are committed to making this notebook and the fine-tuning process as smooth and enjoyable as possible. Your feedback will help us refine and enhance the resources available to the community.\n",
    "Remember, the journey of fine-tuning language models is an iterative and continuous process. Embrace the challenges, celebrate your successes, and continue pushing the boundaries of what's possible. Together, we can unlock the full potential of these powerful models and drive innovation in various domains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de0e50e-4783-4b8f-b00b-ce7891a58b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Modin",
   "language": "python",
   "name": "modin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
